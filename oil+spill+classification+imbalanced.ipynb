{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many imbalanced classification tasks require a skillful model that predicts a crisp class label,\n",
    "where both classes are equally important. An example of an imbalanced classification problem\n",
    "where a class label is required and both classes are equally important is the detection of oil spills\n",
    "or slicks in satellite images. The detection of a spill requires mobilizing an expensive response,\n",
    "and missing an event is equally expensive, causing damage to the environment.\n",
    "\n",
    "One way to evaluate imbalanced classification models that predict crisp labels is to calculate\n",
    "the separate accuracy on the positive class and the negative class, referred to as sensitivity and\n",
    "specificity. These two measures can then be averaged using the geometric mean, referred to as\n",
    "the G-mean, that is insensitive to the skewed class distribution and correctly reports on the\n",
    "skill of the model on both classes. In this tutorial, I have explored how to develop a model to\n",
    "predict the presence of an oil spill in satellite images and evaluate it using the G-mean metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is divided into five parts; they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Oil Spill Dataset](#Oil-Spill-Dataset)\n",
    "2. [Explore the Dataset](#Explore-the-Dataset)\n",
    "3. [Model Test and Baseline Result](#Model-Test-and-Baseline-Result)\n",
    "4. [Evaluate Models](#Evaluate-Models)\n",
    "5. [Make Prediction on New Data](#Make-Prediction-on-New-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oil Spill Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use a standard imbalanced machine learning dataset referred to as the\n",
    "oil spill dataset, oil slicks dataset or simply oil. The dataset was introduced in the 1998 paper\n",
    "by Miroslav Kubat, et al. titled Machine Learning for the Detection of Oil Spills in Satellite\n",
    "Radar Images. The dataset is often credited to Robert Holte, a co-author of the paper. The\n",
    "dataset was developed by starting with satellite images of the ocean, some of which contain an\n",
    "oil spill and some that do not. Images were split into sections and processed using computer\n",
    "vision algorithms to provide a vector of features to describe the contents of the image section or\n",
    "patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task, a model is given a vector that describes the contents of a patch of a satellite\n",
    "image, then predicts whether the patch contains an oil spill or not, e.g. from the illegal or\n",
    "accidental dumping of oil in the ocean. There are 937 cases. Each case is comprised of 48\n",
    "numerical computer vision derived features, a patch number, and a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of nine satellite images were processed into patches. Cases in the dataset are ordered\n",
    "by image and the \f",
    "rst column of the dataset represents the patch number for the image. This\n",
    "was provided for the purposes of estimating model performance per-image. In this case, we are\n",
    "not interested in the image or patch number and this first column can be removed. The normal\n",
    "case is no oil spill assigned the class label of 0, whereas an oil spill is indicated by a class label\n",
    "of 1. There are 896 cases for no oil spill and 41 cases of an oil spill.\n",
    "\n",
    "**Dataset**: https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# compare probabilistic model on the oil spill dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file as a data frame\n",
    "dataframe = read_csv(full_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 47)\n"
     ]
    }
   ],
   "source": [
    "# summarize the shape of the dataset\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the class distribution\n",
    "target = dataframe.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=1, Count=41, Percentage=4.376%\n",
      "Class=0, Count=896, Percentage=95.624%\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(target)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(target) * 100\n",
    "    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 56 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a histogram plot of each variable\n",
    "ax = dataframe.hist()\n",
    "# disable axis labels\n",
    "for axis in ax.flatten():\n",
    "    axis.set_title('')\n",
    "    axis.set_xticklabels([])\n",
    "    axis.set_yticklabels([])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test and Baseline Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate candidate models using repeated stratified k-fold cross-validation. The k-fold\n",
    "cross-validation procedure provides a good general estimate of model performance that is not too\n",
    "optimistically biased, at least compared to a single train-test split. We will use k=10, meaning\n",
    "each fold will contain about 937/10 or about 94 examples. Stratified means that each fold will\n",
    "contain the same mixture of examples by class, that is about 96% to 4% non-spill and spill.\n",
    "Repeated means that the evaluation process will be performed multiple times to help avoid \n",
    "fluke results and better capture the variance of the chosen model. We will use three repeats. This\n",
    "means a single model will be fit and evaluated 10 x 3 (30) times and the mean and standard\n",
    "deviation of these runs will be reported.\n",
    "\n",
    "This can be achieved using the RepeatedStratifiedKFold scikit-learn class. We are pre-\n",
    "dicting class labels of whether a satellite image patch contains a spill or not. There are many\n",
    "measures we could use, although the authors of the paper chose to report the sensitivity,\n",
    "specificity, and the geometric mean of the two scores, called the G-mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity is a measure of the accuracy for the positive class and specificity\n",
    "is a measure of the accuracy of the negative class. The G-mean seeks a balance of these scores,\n",
    "the geometric mean, where poor performance for one or the other results in a low G-mean score. We can calculate the G-mean for a set of predictions made by a model using the geometric mean score() function provided by the imbalanced-learn\n",
    "library. First, we can define a function to load the dataset and split the columns into input and\n",
    "output variables. We will also drop column 22 because the column contains a single value, and\n",
    "the first column that defines the image patch number. The ``load_dataset()`` function below\n",
    "implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    data = read_csv(full_path, header=None)\n",
    "    # drop unused columns\n",
    "    data.drop(22, axis=1, inplace=True)\n",
    "    data.drop(0, axis=1, inplace=True)\n",
    "    # retrieve numpy array\n",
    "    data = data.values\n",
    "    # split into input and output elements\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a function that will evaluate a given model on the dataset and return a list\n",
    "of G-mean scores for each fold and repeat. The ``evaluate_model()`` function below implements\n",
    "this, taking the dataset and model as arguments and returning the list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation the metric\n",
    "    metric = make_scorer(geometric_mean_score)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate a baseline model on the dataset using this test harness. A model that\n",
    "predicts the majority class label (0) or the minority class label (1) for all cases will result in a\n",
    "G-mean of zero. As such, a good default strategy would be to randomly predict one class label or\n",
    "another with a 50% probability and aim for a G-mean of about 0.5. This can be achieved using the DummyClassifier class from the scikit-learn library and setting the strategy argument to\n",
    "``uniform``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reference model\n",
    "model = DummyClassifier(strategy='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is evaluated, we can report the mean and standard deviation of the G-mean\n",
    "scores directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'oil-spill.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X, y = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean G-mean: 0.461 (0.134)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean G-mean: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code first loads and summarizes the dataset. We can see that we have the\n",
    "correct number of rows loaded, and that we have 47 computer vision derived input variables,\n",
    "with the constant value column (index 22) and the patch number column (index 0) removed.\n",
    "Importantly, we can see that the class labels have the correct mapping to integers with 0 for\n",
    "the majority class and 1 for the minority class, customary for imbalanced binary classification\n",
    "dataset. Next, the average of the G-mean scores is reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the baseline algorithm achieves a G-mean of about 0.47, close to\n",
    "the theoretical maximum of 0.5. This score provides a lower limit on model skill; any model that\n",
    "achieves an average G-mean above about 0.47 (or really above 0.5) has skill, whereas models\n",
    "that achieve a score below this value do not have skill on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluate Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by evaluating some probabilistic models on the dataset. Probabilistic models are\n",
    "those models that are fit on the data under a probabilistic framework and often perform well\n",
    "in general for imbalanced classification datasets. We will evaluate the following probabilistic\n",
    "models with default hyperparameters in the dataset:\n",
    "\n",
    "1. Logistic Regression (LR)\n",
    "\n",
    "2. Linear Discriminant Analysis (LDA)\n",
    "\n",
    "3. Gaussian Naive Bayes (NB)\n",
    "\n",
    "Both LR and LDA are sensitive to the scale of the input variables, and often expect and/or\n",
    "perform better if input variables with different scales are normalized or standardized as a\n",
    "pre-processing step. In this case, we will standardize the dataset prior to fitting each model.\n",
    "This will be achieved using a Pipeline and the StandardScaler class. The use of a Pipeline\n",
    "ensures that the StandardScaler is fit on the training dataset and applied to the train and\n",
    "test sets within each k-fold cross-validation evaluation, avoiding any data leakage that might\n",
    "result in an optimistic result. We can define a function to create the models to evaluate on our\n",
    "test harness as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # LR\n",
    "    steps = [('t',StandardScaler()),('m',LogisticRegression(solver='liblinear'))]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('LR')\n",
    "    # LDA\n",
    "    steps = [('t', StandardScaler()),('m',LinearDiscriminantAnalysis())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('LDA')\n",
    "    # NB\n",
    "    models.append(GaussianNB())\n",
    "    names.append('NB')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "models, names = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined, we can enumerate the list and evaluate each in turn. The mean and standard\n",
    "deviation of G-mean scores can be printed during evaluation and the sample of scores can be\n",
    "stored. Algorithms can be compared directly based on their mean G-mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.672 (0.203)\n",
      ">LDA 0.755 (0.147)\n",
      ">NB 0.707 (0.202)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model\n",
    "results = list()\n",
    "for i in range(len(models)):\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, models[i])\n",
    "    results.append(scores)\n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we can use the scores to create a box and whisker plot for each\n",
    "algorithm. Creating the plots side by side allows the distributions to be compared both with\n",
    "regard to the mean score, but also the middle 50 percent of the distribution between the 25th\n",
    "and 75th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that each algorithm has\n",
    "skill, achieving a mean G-mean above 0.5. The results suggest that an LDA might be the best\n",
    "performing of the models tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the G-mean scores is summarized using a figure with a box and whisker\n",
    "plot for each algorithm. We can see that the distribution for both LDA and NB is compact and skillful and that the LR may have a few results during the run where the method performed\n",
    "poorly, pushing the distribution down. This highlights that it is not just the mean performance,\n",
    "but also the consistency of the model that should be considered when selecting a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate Balanced Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression algorithm supports a modification that adjusts the importance of\n",
    "classification errors to be inversely proportional to the class weighting. This allows the model\n",
    "to better learn the class boundary in favor of the minority class, which might help overall\n",
    "G-mean performance. We can achieve this by setting the ``class_weight`` argument of the\n",
    "``LogisticRegression`` to ``balanced``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(solver='liblinear', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is sensitive to the scale of input variables and can perform\n",
    "better with normalized or standardized inputs; as such it is a good idea to test both for a given\n",
    "dataset. Additionally, a power distribution can be used to spread out the distribution of each input variable and make those variables with a Gaussian-like distribution more Gaussian. This\n",
    "can benefit models like Logistic Regression that make assumptions about the distribution of\n",
    "input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power transform will use the Yeo-Johnson method that supports positive and negative\n",
    "inputs, but we will also normalize data prior to the transform. Also, the ``PowerTransformer``\n",
    "class used for the transform will also standardize each variable after the transform. We will\n",
    "compare a ``LogisticRegression`` with a balanced class weighting to the same algorithm with\n",
    "three difierent data preparation schemes, specifically normalization, standardization, and a\n",
    "power transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # LR Balanced\n",
    "    models.append(LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
    "    names.append('Balanced')\n",
    "    # LR Balanced + Normalization\n",
    "    steps = [('t',MinMaxScaler()), ('m', LogisticRegression(solver='liblinear',\n",
    "    class_weight='balanced'))]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Balanced-Norm')\n",
    "    # LR Balanced + Standardization\n",
    "    steps = [('t',StandardScaler()), ('m', LogisticRegression(solver='liblinear',\n",
    "    class_weight='balanced'))]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Balanced-Std')\n",
    "    # LR Balanced + Power\n",
    "    steps = [('t1',MinMaxScaler()), ('t2',PowerTransformer()), ('m',\n",
    "    LogisticRegression(solver='liblinear', class_weight='balanced'))]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Balanced-Power')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Balanced 0.843 (0.123)\n",
      ">Balanced-Norm 0.836 (0.088)\n",
      ">Balanced-Std 0.834 (0.129)\n",
      ">Balanced-Power 0.862 (0.124)\n"
     ]
    }
   ],
   "source": [
    "# define models\n",
    "models, names = get_models()\n",
    "# evaluate each model\n",
    "results = list()\n",
    "for i in range(len(models)):\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, models[i])\n",
    "    results.append(scores)\n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the balanced version of logistic regression performs much better\n",
    "than all of the probabilistic models evaluated in the previous section. The results suggest that\n",
    "perhaps the use of balanced LR with data normalization for pre-processing performs the best\n",
    "on this dataset with a mean G-mean score of about 0.852."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution for the balanced LR is tighter in\n",
    "general than the non-balanced version in the previous section. We can also see that the median\n",
    "result (orange line) for the normalized version is higher than the mean, above 0.9, which is\n",
    "impressive. A mean different from the median suggests a skewed distribution for the results,\n",
    "pulling the mean down with a few bad outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Data Sampling With Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sampling provides a way to better prepare the imbalanced training dataset prior to fitting\n",
    "a model. Perhaps the most popular data sampling is the **SMOTE** oversampling technique for\n",
    "creating new synthetic examples for the minority class. This can be paired with the edited\n",
    "nearest neighbor (ENN) algorithm that will locate and remove examples from the dataset that\n",
    "are ambiguous, making it easier for models to learn to discriminate between the two classes.\n",
    "This combination is called **SMOTE-ENN** and can be implemented using the ``SMOTEENN`` class\n",
    "from the imbalanced-learn library; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SMOTE-ENN data sampling method\n",
    "e = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE** and **ENN**\n",
    "both work better when the input data is scaled beforehand. This\n",
    "is because both techniques involve using the nearest neighbor algorithm internally and this\n",
    "algorithm is sensitive to input variables with different scales. Therefore, we will require the data to be normalized as a first step, then sampled, then used as input to the (unbalanced) logistic\n",
    "regression model. As such, we can use the Pipeline class provided by the imbalanced-learn\n",
    "library to create a sequence of data transforms including the data sampling method, and ending\n",
    "with the logistic regression model. We will compare four variations of the logistic regression\n",
    "model with data sampling, specifically:\n",
    "\n",
    "i. SMOTEENN + LR\n",
    "\n",
    "ii. Normalization + SMOTEENN + LR\n",
    "\n",
    "iii. Standardization + SMOTEENN + LR\n",
    "\n",
    "iv. Normalization + Power + SMOTEENN + LR\n",
    "\n",
    "The expectation is that LR will perform better with SMOTEENN, and that SMOTEENN\n",
    "will perform better with standardization or normalization. The last case does a lot, first\n",
    "normalizing the dataset, then applying the power transform, standardizing the result (the PowerTransformer class will standardize the output by default), applying SMOTEENN,\n",
    "then finally fitting a logistic regression model. These combinations can be defined as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # SMOTEENN\n",
    "    sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    steps = [('e', sampling), ('m', model)]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('LR')\n",
    "    # SMOTEENN + Norm\n",
    "    sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    steps = [('t', MinMaxScaler()), ('e', sampling), ('m', model)]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Norm')\n",
    "    # SMOTEENN + Std\n",
    "    sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    steps = [('t', StandardScaler()), ('e', sampling), ('m', model)]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Std')\n",
    "    # SMOTEENN + Power\n",
    "    sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    steps = [('t1', MinMaxScaler()), ('t2', PowerTransformer()), ('e', sampling), ('m',\n",
    "    model)]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('Power')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.862 (0.094)\n",
      ">Norm 0.829 (0.089)\n",
      ">Std 0.830 (0.122)\n",
      ">Power 0.883 (0.121)\n"
     ]
    }
   ],
   "source": [
    "# define models\n",
    "models, names = get_models()\n",
    "# evaluate each model\n",
    "results = list()\n",
    "for i in range(len(models)):\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, models[i])\n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "    results.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the addition of SMOTEENN improves the performance of the\n",
    "default LR algorithm, achieving a mean G-mean of 0.852 compared to 0.621 seen in the \f",
    "rst\n",
    "set of experimental results. This is even better than balanced LR without any data scaling\n",
    "(previous section) that achieved a G-mean of about 0.846.\n",
    "\n",
    "The results suggest that perhaps the final combination of normalization, power transform,\n",
    "and standardization achieves a slightly better score than the default LR with SMOTEENN with\n",
    "a G-mean of about 0.873."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of results can be compared with box and whisker plots. We can see the\n",
    "distributions all roughly have the same tight spread and that the difference in means of the\n",
    "results can be used to select a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of SMOTEENN with Logistic Regression directly without any data scaling probably\n",
    "provides the simplest and well-performing model that could be used going forward. This model\n",
    "had a mean G-mean of about 0.852 on our test harness. We will use this as our final model and\n",
    "use it to make predictions on new data. First, we can define the model as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "smoteenn = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "pipeline = Pipeline(steps=[('e', smoteenn), ('m', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>1627.54</td>\n",
       "      <td>1409.43</td>\n",
       "      <td>51</td>\n",
       "      <td>822500</td>\n",
       "      <td>35.00</td>\n",
       "      <td>6.10</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.17</td>\n",
       "      <td>178.4</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>1460.31</td>\n",
       "      <td>710.63</td>\n",
       "      <td>451.78</td>\n",
       "      <td>150.85</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0</td>\n",
       "      <td>4530.75</td>\n",
       "      <td>66.25</td>\n",
       "      <td>7.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3234</td>\n",
       "      <td>1091.56</td>\n",
       "      <td>1357.96</td>\n",
       "      <td>32</td>\n",
       "      <td>8085000</td>\n",
       "      <td>40.08</td>\n",
       "      <td>8.98</td>\n",
       "      <td>25450</td>\n",
       "      <td>0.22</td>\n",
       "      <td>317.7</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>4287.77</td>\n",
       "      <td>3095.56</td>\n",
       "      <td>1937.42</td>\n",
       "      <td>773.69</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0</td>\n",
       "      <td>4927.51</td>\n",
       "      <td>66.15</td>\n",
       "      <td>7.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2339</td>\n",
       "      <td>1537.68</td>\n",
       "      <td>1633.02</td>\n",
       "      <td>45</td>\n",
       "      <td>5847500</td>\n",
       "      <td>38.13</td>\n",
       "      <td>9.29</td>\n",
       "      <td>22110</td>\n",
       "      <td>0.24</td>\n",
       "      <td>264.5</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>3959.80</td>\n",
       "      <td>2404.16</td>\n",
       "      <td>1530.38</td>\n",
       "      <td>659.67</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0</td>\n",
       "      <td>4732.04</td>\n",
       "      <td>66.34</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1        2   3        4      5     6      7     8      9   ...  \\\n",
       "0   329  1627.54  1409.43  51   822500  35.00  6.10   4610  0.17  178.4  ...   \n",
       "1  3234  1091.56  1357.96  32  8085000  40.08  8.98  25450  0.22  317.7  ...   \n",
       "2  2339  1537.68  1633.02  45  5847500  38.13  9.29  22110  0.24  264.5  ...   \n",
       "\n",
       "   38       39       40       41      42    43  44       45     46    47  \n",
       "0  55  1460.31   710.63   451.78  150.85  3.23   0  4530.75  66.25  7.85  \n",
       "1  55  4287.77  3095.56  1937.42  773.69  2.21   0  4927.51  66.15  7.24  \n",
       "2  55  3959.80  2404.16  1530.38  659.67  2.59   0  4732.04  66.34  7.67  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, we can use it to make predictions for new data by calling the ``predict()`` function.\n",
    "This will return the class label of 0 for no oil spill, or 1 for an oil spill. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Spill Cases:\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "# evaluate on some non-spill cases (known class 0)\n",
    "print('Non-Spill Cases:')\n",
    "data = [[1627.54, 1409.43, 51, 822500, 35, 6.1, 4610, 0.17, 178.4, 0.2, 0.24, 0.39,\n",
    "0.12, 0.27, 138.32, 34.81, 2.02, 0.14, 0.19, 75.26, 0, 0.47, 351.67, 0.18, 9.24, 0.38,\n",
    "2.57, -2.96, -0.28, 1.93, 0, 1.93, 34, 1710, 0, 25.84, 78, 55, 1460.31, 710.63, 451.78,\n",
    "150.85, 3.23, 0, 4530.75, 66.25, 7.85],\n",
    "[1091.56, 1357.96, 32, 8085000, 40.08, 8.98, 25450, 0.22, 317.7, 0.18, 0.2, 0.49,\n",
    "0.09, 0.41, 114.69, 41.87, 2.31, 0.15, 0.18, 75.26, 0, 0.53, 351.67, 0.18, 9.24,\n",
    "0.24, 3.56, -3.09, -0.31, 2.17, 0, 2.17, 281, 14490, 0, 80.11, 78, 55, 4287.77,\n",
    "3095.56, 1937.42, 773.69, 2.21, 0, 4927.51, 66.15, 7.24],\n",
    "[1537.68, 1633.02, 45, 5847500, 38.13, 9.29, 22110, 0.24, 264.5, 0.21, 0.26, 0.79,\n",
    "0.08, 0.71, 89.49, 32.23, 2.2, 0.17, 0.22, 75.26, 0, 0.51, 351.67, 0.18, 9.24, 0.27,\n",
    "4.21, -2.84, -0.29, 2.16, 0, 2.16, 228, 12150, 0, 83.6, 78, 55, 3959.8, 2404.16,\n",
    "1530.38, 659.67, 2.59, 0, 4732.04, 66.34, 7.67]]\n",
    "\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 0)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spill Cases:\n",
      ">Predicted=0 (expected 1)\n",
      ">Predicted=0 (expected 1)\n",
      ">Predicted=0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "print('Spill Cases:')\n",
    "data = [[1020.91, 630.8, 59, 7427500, 32.76, 10.48, 17380, 0.32, 427.4, 0.22, 0.29,\n",
    "0.5, 0.08, 0.42, 149.87, 50.99, 1.89, 0.14, 0.18, 75.26, 0, 0.44, 351.67, 0.18, 9.24,\n",
    "2.5, 10.63, -3.07, -0.28, 2.18, 0, 2.18, 164, 8730, 0, 40.67, 78, 55, 5650.88, 1749.29,\n",
    "1245.07, 348.7, 4.54, 0, 25579.34, 65.78, 7.41],\n",
    "[1118.08, 469.39, 11, 7887500, 30.41, 7.99, 15880, 0.26, 496.7, 0.2, 0.26, 0.69,\n",
    "0.11, 0.58, 118.11, 43.96, 1.76, 0.15, 0.18, 75.26, 0, 0.4, 351.67, 0.18, 9.24, 0.78,\n",
    "8.68, -3.19, -0.33, 2.19, 0, 2.19, 150, 8100, 0, 31.97, 78, 55, 3471.31, 3059.41,\n",
    "2043.9, 477.23, 1.7, 0, 28172.07, 65.72, 7.58],\n",
    "[1449.85, 608.43, 88, 287500, 40.42, 7.34, 3340, 0.18, 86.1, 0.21, 0.32, 0.5, 0.17,\n",
    "0.34, 71.2, 16.73, 1.82, 0.19, 0.29, 87.65, 0, 0.46, 132.78, -0.01, 3.78, 0.7, 4.79,\n",
    "-3.36, -0.23, 1.95, 0, 1.95, 29, 1530, 0.01, 38.8, 89, 69, 1400, 250, 150, 45.13,\n",
    "9.33, 1, 31692.84, 65.81, 7.84]]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first fits the model on the entire training dataset. Then the fit model\n",
    "used to predict the label of an oil spill for cases where we know there is none, chosen from the\n",
    "dataset file. We can see that all cases are correctly predicted. Then some cases of actual oil\n",
    "spills are used as input to the model and the label is predicted. As we might have hoped, the\n",
    "correct labels are again predicted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
